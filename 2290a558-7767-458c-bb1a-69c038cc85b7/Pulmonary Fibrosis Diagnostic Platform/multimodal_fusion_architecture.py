import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Multimodal Fusion Architecture for FVC Prediction
print("=" * 70)
print("MULTIMODAL FUSION ARCHITECTURE")
print("=" * 70)

# Define fusion strategies
print("\nüîÄ FUSION STRATEGIES")
print("-" * 70)

fusion_strategies = {
    'Early Fusion (Concatenation)': {
        'description': 'Concatenate all modality embeddings and pass through fusion network',
        'input_dims': [128, 512, 768],  # Dense, CNN, BERT
        'concatenated_dim': 128 + 512 + 768,
        'architecture': 'Concat ‚Üí FC(1408‚Üí512) ‚Üí BN ‚Üí ReLU ‚Üí Dropout(0.3) ‚Üí FC(512‚Üí256)',
        'pros': ['Simple', 'Effective when all modalities are available', 'Fast inference'],
        'cons': ['All modalities required', 'No inter-modal attention'],
        'params': (1408 * 512 + 512) + (512 * 2) + (512 * 256 + 256)
    },
    'Late Fusion (Weighted Average)': {
        'description': 'Independent predictions from each modality, then weighted combination',
        'input_dims': [128, 512, 768],
        'architecture': 'Each modality ‚Üí prediction head ‚Üí weighted average',
        'pros': ['Modalities can be missing', 'Interpretable weights', 'Modular'],
        'cons': ['No cross-modal interactions', 'More parameters'],
        'params': (128 * 64 + 64 + 64 + 1) + (512 * 64 + 64 + 64 + 1) + (768 * 64 + 64 + 64 + 1)
    },
    'Attention-Based Fusion': {
        'description': 'Cross-modal attention to learn relationships between modalities',
        'input_dims': [128, 512, 768],
        'architecture': 'Multi-head attention ‚Üí projection ‚Üí fusion MLP',
        'pros': ['Learns inter-modal relationships', 'Adaptive importance', 'State-of-the-art'],
        'cons': ['More complex', 'More parameters', 'Slower'],
        'params': 'Variable based on attention heads (estimated: ~1.5M)'
    }
}

for strategy_name, config in fusion_strategies.items():
    print(f"\nüîπ {strategy_name}")
    print(f"   Description: {config['description']}")
    print(f"   Architecture: {config['architecture']}")
    print(f"   Pros: {', '.join(config['pros'])}")
    print(f"   Cons: {', '.join(config['cons'])}")

# Recommended: Attention-Based Fusion
print("\n\nüéØ RECOMMENDED: ATTENTION-BASED FUSION")
print("-" * 70)

class MultimodalAttentionFusion:
    """
    Attention-based multimodal fusion architecture.
    Uses cross-modal attention to learn relationships between modalities.
    """
    def __init__(self, 
                 lab_dim=128, 
                 image_dim=512, 
                 text_dim=768,
                 fusion_dim=256,
                 num_heads=4,
                 output_dim=1):
        
        self.lab_dim = lab_dim
        self.image_dim = image_dim
        self.text_dim = text_dim
        self.fusion_dim = fusion_dim
        self.num_heads = num_heads
        self.output_dim = output_dim
        
        # Project all modalities to same dimension
        self.projection_params = {
            'lab_projection': lab_dim * fusion_dim + fusion_dim,
            'image_projection': image_dim * fusion_dim + fusion_dim,
            'text_projection': text_dim * fusion_dim + fusion_dim
        }
        
        # Multi-head cross-attention parameters
        # Q, K, V projections for each head
        self.attention_params = {
            'query_projection': fusion_dim * fusion_dim * num_heads,
            'key_projection': fusion_dim * fusion_dim * num_heads,
            'value_projection': fusion_dim * fusion_dim * num_heads,
            'output_projection': fusion_dim * num_heads * fusion_dim
        }
        
        # Fusion network parameters
        self.fusion_params = {
            'fusion_layer1': fusion_dim * 512 + 512,
            'fusion_bn1': 512 * 2,
            'fusion_layer2': 512 * 256 + 256,
            'fusion_bn2': 256 * 2,
            'fusion_layer3': 256 * 128 + 128,
            'fusion_bn3': 128 * 2
        }
        
        # Regression head parameters
        self.regression_params = {
            'reg_layer1': 128 * 64 + 64,
            'reg_layer2': 64 * 32 + 32,
            'reg_output': 32 * output_dim + output_dim
        }
        
    def count_parameters(self):
        """Calculate total parameters"""
        total = 0
        
        # Projection layers
        total += sum(self.projection_params.values())
        
        # Attention layers
        total += sum(self.attention_params.values())
        
        # Fusion network
        total += sum(self.fusion_params.values())
        
        # Regression head
        total += sum(self.regression_params.values())
        
        return total
    
    def get_architecture_summary(self):
        return {
            'modalities': {
                'structured_lab': f'{self.lab_dim}-dim',
                'ct_images': f'{self.image_dim}-dim',
                'clinical_text': f'{self.text_dim}-dim'
            },
            'projection_layers': {
                'lab': f'{self.lab_dim} ‚Üí {self.fusion_dim}',
                'image': f'{self.image_dim} ‚Üí {self.fusion_dim}',
                'text': f'{self.text_dim} ‚Üí {self.fusion_dim}'
            },
            'attention': {
                'type': 'Multi-head cross-modal attention',
                'num_heads': self.num_heads,
                'hidden_dim': self.fusion_dim
            },
            'fusion_network': f'{self.fusion_dim} ‚Üí 512 ‚Üí 256 ‚Üí 128',
            'regression_head': '128 ‚Üí 64 ‚Üí 32 ‚Üí 1',
            'total_parameters': self.count_parameters()
        }

# Initialize fusion model
print("\nüèóÔ∏è BUILDING ATTENTION-BASED FUSION MODEL")

fusion_model = MultimodalAttentionFusion(
    lab_dim=128,
    image_dim=512,
    text_dim=768,
    fusion_dim=256,
    num_heads=4,
    output_dim=1
)

fusion_arch = fusion_model.get_architecture_summary()

print(f"\nüìä Architecture Summary:")
print(f"   Modalities:")
print(f"      ‚Ä¢ Structured Lab: {fusion_arch['modalities']['structured_lab']} embedding")
print(f"      ‚Ä¢ CT Images: {fusion_arch['modalities']['ct_images']} embedding")
print(f"      ‚Ä¢ Clinical Text: {fusion_arch['modalities']['clinical_text']} embedding")
print(f"\n   Projection Layers:")
print(f"      ‚Ä¢ Lab: {fusion_arch['projection_layers']['lab']}")
print(f"      ‚Ä¢ Image: {fusion_arch['projection_layers']['image']}")
print(f"      ‚Ä¢ Text: {fusion_arch['projection_layers']['text']}")
print(f"\n   Cross-Modal Attention:")
print(f"      ‚Ä¢ Type: {fusion_arch['attention']['type']}")
print(f"      ‚Ä¢ Number of Heads: {fusion_arch['attention']['num_heads']}")
print(f"      ‚Ä¢ Hidden Dimension: {fusion_arch['attention']['hidden_dim']}")
print(f"\n   Fusion Network: {fusion_arch['fusion_network']}")
print(f"   Regression Head: {fusion_arch['regression_head']}")
print(f"\n   Total Fusion Parameters: {fusion_arch['total_parameters']:,}")

# Calculate total model parameters (encoders + fusion)
encoder_params = {
    'Dense Lab Encoder': 299648,
    'EfficientNet CT Encoder': 6673284,
    'ClinicalBERT Text Encoder': 110000000,
    'Fusion Network': fusion_arch['total_parameters']
}

total_model_params = sum(encoder_params.values())

print(f"\n\nüìà COMPLETE MODEL PARAMETER COUNT")
print("-" * 70)
for component, params in encoder_params.items():
    pct = (params / total_model_params) * 100
    print(f"   {component}: {params:,} ({pct:.1f}%)")
print(f"\n   TOTAL: {total_model_params:,} parameters")

# Visualize architecture
print("\n\nüìä CREATING FUSION ARCHITECTURE VISUALIZATION")

fusion_fig = plt.figure(figsize=(14, 10), facecolor='#1D1D20')
fusion_fig.suptitle('Multimodal Attention-Based Fusion for FVC Prediction', 
                    fontsize=16, fontweight='bold', color='#fbfbff', y=0.98)

colors = ['#A1C9F4', '#FFB482', '#8DE5A1', '#FF9F9B', '#D0BBFF']
bg_color = '#1D1D20'
text_color = '#fbfbff'
secondary_text = '#909094'

# Create architecture diagram
ax = plt.subplot(1, 1, 1, facecolor=bg_color)
ax.set_xlim(0, 10)
ax.set_ylim(0, 10)
ax.axis('off')

# Input modality encoders (bottom)
encoder_y = 8.5
encoders = [
    ('Lab Data\n128-dim', 1.5, colors[0]),
    ('CT Images\n512-dim', 5, colors[1]),
    ('Clinical Notes\n768-dim', 8.5, colors[2])
]

for label, x_pos, color in encoders:
    from matplotlib.patches import FancyBboxPatch
    rect = FancyBboxPatch((x_pos-0.6, encoder_y-0.4), 1.2, 0.8,
                          boxstyle="round,pad=0.05",
                          edgecolor=color, facecolor=color, linewidth=2, alpha=0.3)
    ax.add_patch(rect)
    ax.text(x_pos, encoder_y, label, ha='center', va='center',
            fontsize=9, color=text_color, weight='bold')

# Projection layers
proj_y = 6.8
ax.text(5, proj_y + 0.8, 'Projection to Common Dimension (256)', 
        ha='center', fontsize=10, color=secondary_text, style='italic')

for x_pos, color in [(1.5, colors[0]), (5, colors[1]), (8.5, colors[2])]:
    rect = FancyBboxPatch((x_pos-0.5, proj_y-0.3), 1.0, 0.6,
                          boxstyle="round,pad=0.05",
                          edgecolor=color, facecolor='none', linewidth=1.5)
    ax.add_patch(rect)
    ax.text(x_pos, proj_y, '256-dim', ha='center', va='center',
            fontsize=8, color=text_color)
    # Arrow from encoder to projection
    ax.arrow(x_pos, encoder_y-0.5, 0, -0.8, head_width=0.15, head_length=0.1,
             fc=color, ec=color, linewidth=2)

# Attention mechanism
attention_y = 5.0
attention_box = FancyBboxPatch((2.5, attention_y-0.6), 5, 1.2,
                               boxstyle="round,pad=0.1",
                               edgecolor='#ffd400', facecolor='#ffd400', 
                               linewidth=2, alpha=0.2)
ax.add_patch(attention_box)
ax.text(5, attention_y + 0.3, 'Multi-Head Cross-Modal Attention', 
        ha='center', fontsize=11, color='#ffd400', weight='bold')
ax.text(5, attention_y - 0.1, '4 Heads √ó 256-dim', 
        ha='center', fontsize=8, color=text_color)

# Arrows from projections to attention
for x_pos, color in [(1.5, colors[0]), (5, colors[1]), (8.5, colors[2])]:
    ax.arrow(x_pos, proj_y-0.4, (5-x_pos)*0.5, -0.7, head_width=0.12, head_length=0.08,
             fc=color, ec=color, linewidth=1.5, alpha=0.7)

# Fusion network
fusion_y = 3.2
fusion_layers = ['512', '256', '128']
fusion_x_start = 3.5
fusion_spacing = 1.2

ax.text(5, fusion_y + 0.8, 'Fusion Network', 
        ha='center', fontsize=10, color=secondary_text, style='italic')

for i, dim in enumerate(fusion_layers):
    x_pos = fusion_x_start + i * fusion_spacing
    rect = FancyBboxPatch((x_pos-0.4, fusion_y-0.3), 0.8, 0.6,
                          boxstyle="round,pad=0.05",
                          edgecolor=colors[3], facecolor='none', linewidth=1.5)
    ax.add_patch(rect)
    ax.text(x_pos, fusion_y, dim, ha='center', va='center',
            fontsize=9, color=text_color, weight='bold')
    
    if i < len(fusion_layers) - 1:
        ax.arrow(x_pos+0.4, fusion_y, fusion_spacing-0.85, 0, 
                head_width=0.12, head_length=0.08,
                fc=colors[3], ec=colors[3], linewidth=1.5)

# Arrow from attention to fusion
ax.arrow(5, attention_y-0.7, 0, -0.5, head_width=0.15, head_length=0.1,
         fc='#ffd400', ec='#ffd400', linewidth=2)

# Regression head
reg_y = 1.5
reg_layers = ['64', '32', '1']
reg_x_start = 3.8
reg_spacing = 1.1

ax.text(5, reg_y + 0.8, 'Regression Head', 
        ha='center', fontsize=10, color=secondary_text, style='italic')

for i, dim in enumerate(reg_layers):
    x_pos = reg_x_start + i * reg_spacing
    if i == len(reg_layers) - 1:
        # Output node
        from matplotlib.patches import Circle
        circle = Circle((x_pos, reg_y), 0.35, edgecolor='#17b26a', 
                       facecolor='#17b26a', linewidth=2, alpha=0.3)
        ax.add_patch(circle)
        ax.text(x_pos, reg_y, 'FVC\nPred', ha='center', va='center',
                fontsize=8, color=text_color, weight='bold')
    else:
        rect = FancyBboxPatch((x_pos-0.3, reg_y-0.25), 0.6, 0.5,
                              boxstyle="round,pad=0.05",
                              edgecolor=colors[4], facecolor='none', linewidth=1.5)
        ax.add_patch(rect)
        ax.text(x_pos, reg_y, dim, ha='center', va='center',
                fontsize=9, color=text_color, weight='bold')
    
    if i < len(reg_layers) - 1:
        ax.arrow(x_pos+0.3, reg_y, reg_spacing-0.65, 0,
                head_width=0.12, head_length=0.08,
                fc=colors[4], ec=colors[4], linewidth=1.5)

# Arrow from fusion to regression
ax.arrow(fusion_x_start + len(fusion_layers)*fusion_spacing - 0.8, fusion_y-0.4, 
         0, -0.4, head_width=0.15, head_length=0.1,
         fc=colors[3], ec=colors[3], linewidth=2)

plt.tight_layout()

print("‚úÖ Fusion architecture visualization created")

# Store configuration
fusion_config = {
    'architecture': 'Attention-Based Multimodal Fusion',
    'modalities': ['structured_lab', 'ct_images', 'clinical_text'],
    'input_dimensions': [128, 512, 768],
    'fusion_dimension': 256,
    'attention_heads': 4,
    'fusion_network_layers': [512, 256, 128],
    'regression_head_layers': [64, 32, 1],
    'total_parameters': total_model_params,
    'fusion_parameters': fusion_arch['total_parameters'],
    'output': 'FVC prediction (continuous value)',
    'ready_for_implementation': True
}

print("\n\n‚úÖ MULTIMODAL FUSION ARCHITECTURE COMPLETE")
print("=" * 70)
print(f"   Architecture: {fusion_config['architecture']}")
print(f"   Modalities: {', '.join(fusion_config['modalities'])}")
print(f"   Fusion Method: {fusion_config['attention_heads']}-head cross-modal attention")
print(f"   Total Parameters: {fusion_config['total_parameters']:,}")
print(f"   Output: {fusion_config['output']}")
print(f"   Status: ‚úÖ Ready for training")
print("=" * 70)
